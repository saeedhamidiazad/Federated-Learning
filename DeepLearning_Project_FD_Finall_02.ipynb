{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zflTIBYyoRd9"
      },
      "source": [
        "# Federated Learning _ Flower (Simulation  with TensorFlow/Keras)\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, we'll simulate a federated learning system with 10 clients. The clients will use TensorFlow/Keras to define model training and evaluation. Let's start by installing Flower (published as `flwr` on PyPI) with the `simulation` extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9GvOhgP_oReH"
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[\"simulation\"] tensorflow\n",
        "!pip install -q flwr_datasets[\"vision\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY5JMZEZoReK"
      },
      "source": [
        "Let's also install Matplotlib so we can make some plots once the simulation is completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yb3aKHNzoReL",
        "outputId": "1ed3e575-f793-4760-e61c-e23df9c34f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0NE0BvHoReM"
      },
      "source": [
        "Next, we import the required dependencies. The most important imports are Flower (`flwr`) and TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "arRQes9CoReN"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "from flwr.simulation.ray_transport.utils import enable_tf_gpu_growth\n",
        "\n",
        "from datasets import Dataset\n",
        "from flwr_datasets import FederatedDataset\n",
        "\n",
        "VERBOSE = 0\n",
        "NUM_CLIENTS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWJJVoaBoReO"
      },
      "source": [
        "Let's start by defining the model we want to federated. Since we will be working with cifar10, using a fully connected model is sufficient. You can of course customize this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bbKJgzWDoReP"
      },
      "outputs": [],
      "source": [
        "def get_model_01():\n",
        "    \"\"\"Constructs a simple model architecture suitable for cifar10.\"\"\"\n",
        "    model = tf.keras.models.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "    model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_02():\n",
        "    \"\"\"Constructs a simple model architecture suitable for cifar10.\"\"\"\n",
        "    model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
        "    model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "gBRIGKc96eq3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def get_model_03():\n",
        "    \"\"\"Constructs a simple model architecture suitable for cifar10.\"\"\"\n",
        "# Define MobileNetV2 model using the pre-trained weights on ImageNet\n",
        "    from tensorflow.keras.applications import MobileNetV2\n",
        "    base_model = MobileNetV2(input_shape=(32, 32, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze convolutional layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "# Create a new model on top\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation=('relu')))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(256, activation=('relu')))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(10, activation=('softmax')))\n",
        "\n",
        "# Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "0AWHWrDel0nx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Constructs a simple model architecture suitable for cifar10.\"\"\"\n",
        "# Define VGG16 model using the pre-trained weights on ImageNet\n",
        "    from tensorflow.keras.applications import VGG16\n",
        "    base_model = VGG16(input_shape=(32, 32, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze convolutional layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "# Create a new model on top\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation=('relu')))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(256, activation=('relu')))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(10, activation=('softmax')))\n",
        "\n",
        "# Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ISRA9t1uUs05"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYxCFsX2oReQ"
      },
      "source": [
        "With that out of the way, let's move on to the interesting bits. Federated learning systems consist of a server and multiple clients. In Flower, we create clients by implementing subclasses of `flwr.client.Client` or `flwr.client.NumPyClient`. We use `NumPyClient` in this tutorial because it is easier to implement and requires us to write less boilerplate.\n",
        "\n",
        "To implement the Flower client, we create a subclass of `flwr.client.NumPyClient` and implement the three methods `get_parameters`, `fit`, and `evaluate`:\n",
        "\n",
        "- `get_parameters`: Return the current local model parameters\n",
        "- `fit`: Receive model parameters from the server, train the model parameters on the local data, and return the (updated) model parameters to the server\n",
        "- `evaluate`: Received model parameters from the server, evaluate the model parameters on the local data, and return the evaluation result to the server\n",
        "\n",
        "We mentioned that our clients will use TensorFlow/Keras for the model training and evaluation. Keras models provide methods that make the implementation straightforward: we can update the local model with server-provides parameters through `model.set_weights`, we can train/evaluate the model through `fit/evaluate`, and we can get the updated model parameters through `model.get_weights`.\n",
        "\n",
        "Let's see a simple implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZqQ12m5doReR"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, trainset, valset) -> None:\n",
        "        # Create model\n",
        "        self.model = get_model()\n",
        "        self.trainset = trainset\n",
        "        self.valset = valset\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        self.model.fit(self.trainset, epochs=1, verbose=VERBOSE)\n",
        "        return self.model.get_weights(), len(self.trainset), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        loss, acc = self.model.evaluate(self.valset, verbose=VERBOSE)\n",
        "        return loss, len(self.valset), {\"accuracy\": acc}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0TLgLbQoReS"
      },
      "source": [
        "Our class `FlowerClient` defines how local training/evaluation will be performed and allows Flower to call the local training/evaluation through `fit` and `evaluate`. Each instance of `FlowerClient` represents a *single client* in our federated learning system. Federated learning systems have multiple clients (otherwise, there's not much to federate, is there?), so each client will be represented by its own instance of `FlowerClient`. If we have, for example, three clients in our workload, we'd have three instances of `FlowerClient`. Flower calls `FlowerClient.fit` on the respective instance when the server selects a particular client for training (and `FlowerClient.evaluate` for evaluation).\n",
        "\n",
        "In this notebook, we want to simulate a federated learning system with 100 clients on a single machine. This means that the server and all 100 clients will live on a single machine and share resources such as CPU, GPU, and memory. Having 100 clients would mean having 100 instances of `FlowerClient` in memory. Doing this on a single machine can quickly exhaust the available memory resources, even if only a subset of these clients participates in a single round of federated learning.\n",
        "\n",
        "In addition to the regular capabilities where server and clients run on multiple machines, Flower, therefore, provides special simulation capabilities that create `FlowerClient` instances only when they are actually necessary for training or evaluation. To enable the Flower framework to create clients when necessary, we need to implement a function called `client_fn` that creates a `FlowerClient` instance on demand. Flower calls `client_fn` whenever it needs an instance of one particular client to call `fit` or `evaluate` (those instances are usually discarded after use). Clients are identified by a client ID, or short `cid`. The `cid` can be used, for example, to load different local data partitions for each client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUCHukD6oReT"
      },
      "source": [
        "We now define four auxiliary functions for this example (note the last two are entirely optional):\n",
        "* `get_client_fn()`: Is a function that returns another function. The returned `client_fn` will be executed by Flower's VirtualClientEngine each time a new _virtual_ client (i.e. a client that is simulated in a Python process) needs to be spawn. When are virtual clients spawned? Each time the strategy samples them to do either `fit()` (i.e. train the global model on the local data of a particular client) or `evaluate()` (i.e. evaluate the global model on the validation set of a given client).\n",
        "\n",
        "* `weighted_average()`: This is an optional function to pass to the strategy. It will be executed after an evaluation round (i.e. when client run `evaluate()`) and will aggregate the metrics clients return. In this example, we use this function to compute the weighted average accuracy of clients doing `evaluate()`.\n",
        "\n",
        "* `get_evaluate_fn()`: This is again a function that returns another function. The returned function will be executed by the strategy at the end of a `fit()` round and after a new global model has been obtained after aggregation. This is an optional argument for Flower strategies. In this example, we use the whole MNIST test set to perform this server-side evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1YZVkAeuoReV"
      },
      "outputs": [],
      "source": [
        "def get_client_fn(dataset: FederatedDataset):\n",
        "    \"\"\"Return a function to construct a client.\n",
        "\n",
        "    The VirtualClientEngine will execute this function whenever a client is sampled by\n",
        "    the strategy to participate.\n",
        "    \"\"\"\n",
        "\n",
        "    def client_fn(cid: str) -> fl.client.Client:\n",
        "        \"\"\"Construct a FlowerClient with its own dataset partition.\"\"\"\n",
        "\n",
        "        # Extract partition for client with id = cid\n",
        "        client_dataset = dataset.load_partition(int(cid), \"train\")\n",
        "\n",
        "        # Now let's split it into train (90%) and validation (10%)\n",
        "        client_dataset_splits = client_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "        trainset = client_dataset_splits[\"train\"].to_tf_dataset(\n",
        "            columns=\"img\", label_cols=\"label\", batch_size=32\n",
        "        )\n",
        "        valset = client_dataset_splits[\"test\"].to_tf_dataset(\n",
        "            columns=\"img\", label_cols=\"label\", batch_size=64\n",
        "        )\n",
        "\n",
        "        # Create and return client\n",
        "        return FlowerClient(trainset, valset).to_client()\n",
        "\n",
        "    return client_fn\n",
        "\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    \"\"\"Aggregation function for (federated) evaluation metrics, i.e. those returned by\n",
        "    the client's evaluate() method.\"\"\"\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
        "\n",
        "\n",
        "def get_evaluate_fn(testset: Dataset):\n",
        "    \"\"\"Return an evaluation function for server-side (i.e. centralised) evaluation.\"\"\"\n",
        "\n",
        "    # The `evaluate` function will be called after every round by the strategy\n",
        "    def evaluate(\n",
        "        server_round: int,\n",
        "        parameters: fl.common.NDArrays,\n",
        "        config: Dict[str, fl.common.Scalar],\n",
        "    ):\n",
        "        model = get_model()  # Construct the model\n",
        "        model.set_weights(parameters)  # Update model with the latest parameters\n",
        "        loss, accuracy = model.evaluate(testset, verbose=VERBOSE)\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2pSYY2OoReX"
      },
      "source": [
        "We now have `FlowerClient` which defines client-side training and evaluation, and `client_fn`, which allows Flower to create `FlowerClient` instances whenever it needs to call `fit` or `evaluate` on one particular client. The last step is to start the actual simulation using `flwr.simulation.start_simulation`.\n",
        "\n",
        "The function `start_simulation` accepts a number of arguments, amongst them the `client_fn` used to create `FlowerClient` instances, the number of clients to simulate `num_clients`, the number of rounds `num_rounds`, and the strategy. The strategy encapsulates the federated learning approach/algorithm, for example, *Federated Averaging* (FedAvg).\n",
        "\n",
        "Flower comes with a number of built-in strategies, but we can also use our own strategy implementations to customize nearly all aspects of the federated learning approach. For this example, we use the built-in `FedAvg` implementation and customize it using a few basic parameters. The last step is the actual call to `start_simulation` which - you guessed it - actually starts the simulation.\n",
        "\n",
        "We can use [Flower Datasets](https://flower.dev/docs/datasets/) to effortlessly obtain an off-the-shelf partitioned dataset or partition one that isn't pre-partitioned. Let's choose MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U4pwE9tjoReY",
        "outputId": "9997193f-70e7-4f43-dd97-033e1eac4957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING flwr 2024-02-01 18:27:11,557 | fedavg.py:117 | \n",
            "Setting `min_available_clients` lower than `min_fit_clients` or\n",
            "`min_evaluate_clients` can cause the server to fail when there are too few clients\n",
            "connected to the server. `min_available_clients` must be set to a value larger\n",
            "than or equal to the values of `min_fit_clients` and `min_evaluate_clients`.\n",
            "\n",
            "WARNING:flwr:\n",
            "Setting `min_available_clients` lower than `min_fit_clients` or\n",
            "`min_evaluate_clients` can cause the server to fail when there are too few clients\n",
            "connected to the server. `min_available_clients` must be set to a value larger\n",
            "than or equal to the values of `min_fit_clients` and `min_evaluate_clients`.\n",
            "\n",
            "INFO flwr 2024-02-01 18:27:11,563 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "2024-02-01 18:27:17,559\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2024-02-01 18:27:22,746 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'memory': 7913044379.0, 'object_store_memory': 3956522188.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'memory': 7913044379.0, 'object_store_memory': 3956522188.0}\n",
            "INFO flwr 2024-02-01 18:27:22,752 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO flwr 2024-02-01 18:27:22,755 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "INFO flwr 2024-02-01 18:27:22,866 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "INFO flwr 2024-02-01 18:27:22,875 | server.py:89 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2024-02-01 18:27:22,882 | server.py:276 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=155238)\u001b[0m 2024-02-01 18:27:33.598608: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=155238)\u001b[0m 2024-02-01 18:27:33.598718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=155238)\u001b[0m 2024-02-01 18:27:33.602657: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=155238)\u001b[0m 2024-02-01 18:27:43.510749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(pid=155237)\u001b[0m 2024-02-01 18:27:34.189356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=155237)\u001b[0m 2024-02-01 18:27:34.189458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=155237)\u001b[0m 2024-02-01 18:27:34.193886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO flwr 2024-02-01 18:27:50,417 | server.py:280 | Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "INFO flwr 2024-02-01 18:27:50,421 | server.py:91 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2024-02-01 18:30:14,085 | server.py:94 | initial parameters (loss, other metrics): 17.055753707885742, {'accuracy': 0.07819999754428864}\n",
            "INFO:flwr:initial parameters (loss, other metrics): 17.055753707885742, {'accuracy': 0.07819999754428864}\n",
            "INFO flwr 2024-02-01 18:30:14,091 | server.py:104 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2024-02-01 18:30:14,099 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 10 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 18:40:47,662 | server.py:236 | fit_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 10 results and 0 failures\n",
            "WARNING flwr 2024-02-01 18:40:50,002 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "INFO flwr 2024-02-01 18:42:57,193 | server.py:125 | fit progress: (1, 1.9129927158355713, {'accuracy': 0.3513999879360199}, 763.0950109240002)\n",
            "INFO:flwr:fit progress: (1, 1.9129927158355713, {'accuracy': 0.3513999879360199}, 763.0950109240002)\n",
            "DEBUG flwr 2024-02-01 18:42:57,202 | server.py:173 | evaluate_round 1: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 5 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 18:43:36,507 | server.py:187 | evaluate_round 1 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 5 results and 0 failures\n",
            "DEBUG flwr 2024-02-01 18:43:36,511 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 10 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 18:54:11,785 | server.py:236 | fit_round 2 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 10 results and 0 failures\n",
            "INFO flwr 2024-02-01 18:56:37,598 | server.py:125 | fit progress: (2, 1.8248263597488403, {'accuracy': 0.38749998807907104}, 1583.4998779319976)\n",
            "INFO:flwr:fit progress: (2, 1.8248263597488403, {'accuracy': 0.38749998807907104}, 1583.4998779319976)\n",
            "DEBUG flwr 2024-02-01 18:56:37,603 | server.py:173 | evaluate_round 2: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 5 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 18:57:22,068 | server.py:187 | evaluate_round 2 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 5 results and 0 failures\n",
            "DEBUG flwr 2024-02-01 18:57:22,074 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 10 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:07:41,219 | server.py:236 | fit_round 3 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 3 received 10 results and 0 failures\n",
            "INFO flwr 2024-02-01 19:10:06,229 | server.py:125 | fit progress: (3, 1.5837198495864868, {'accuracy': 0.4828000068664551}, 2392.130711101996)\n",
            "INFO:flwr:fit progress: (3, 1.5837198495864868, {'accuracy': 0.4828000068664551}, 2392.130711101996)\n",
            "DEBUG flwr 2024-02-01 19:10:06,234 | server.py:173 | evaluate_round 3: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 5 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:10:45,224 | server.py:187 | evaluate_round 3 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 5 results and 0 failures\n",
            "DEBUG flwr 2024-02-01 19:10:45,231 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 4: strategy sampled 10 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:21:04,592 | server.py:236 | fit_round 4 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 4 received 10 results and 0 failures\n",
            "INFO flwr 2024-02-01 19:23:29,381 | server.py:125 | fit progress: (4, 1.4736454486846924, {'accuracy': 0.5307000279426575}, 3195.2828178279997)\n",
            "INFO:flwr:fit progress: (4, 1.4736454486846924, {'accuracy': 0.5307000279426575}, 3195.2828178279997)\n",
            "DEBUG flwr 2024-02-01 19:23:29,389 | server.py:173 | evaluate_round 4: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 4: strategy sampled 5 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:24:07,733 | server.py:187 | evaluate_round 4 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 4 received 5 results and 0 failures\n",
            "DEBUG flwr 2024-02-01 19:24:07,737 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 5: strategy sampled 10 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:34:40,953 | server.py:236 | fit_round 5 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 5 received 10 results and 0 failures\n",
            "INFO flwr 2024-02-01 19:37:06,139 | server.py:125 | fit progress: (5, 1.445074439048767, {'accuracy': 0.5415999889373779}, 4012.0401613629983)\n",
            "INFO:flwr:fit progress: (5, 1.445074439048767, {'accuracy': 0.5415999889373779}, 4012.0401613629983)\n",
            "DEBUG flwr 2024-02-01 19:37:06,145 | server.py:173 | evaluate_round 5: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 5: strategy sampled 5 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:37:48,566 | server.py:187 | evaluate_round 5 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 5 received 5 results and 0 failures\n",
            "DEBUG flwr 2024-02-01 19:37:48,571 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 6: strategy sampled 10 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:48:22,954 | server.py:236 | fit_round 6 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 6 received 10 results and 0 failures\n",
            "INFO flwr 2024-02-01 19:50:49,327 | server.py:125 | fit progress: (6, 1.400529384613037, {'accuracy': 0.5583999752998352}, 4835.228914272997)\n",
            "INFO:flwr:fit progress: (6, 1.400529384613037, {'accuracy': 0.5583999752998352}, 4835.228914272997)\n",
            "DEBUG flwr 2024-02-01 19:50:49,333 | server.py:173 | evaluate_round 6: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 6: strategy sampled 5 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 19:51:28,843 | server.py:187 | evaluate_round 6 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 6 received 5 results and 0 failures\n",
            "DEBUG flwr 2024-02-01 19:51:28,848 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 7: strategy sampled 10 clients (out of 10)\n",
            "DEBUG flwr 2024-02-01 20:01:50,870 | server.py:236 | fit_round 7 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 7 received 10 results and 0 failures\n",
            "2024-02-01 20:01:53,942\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffffa173e8fc9c625b37cec03a701000000 Worker ID: 26177de7b16419f652bd7b02f771c7c2ce627a8d3f6abd90e78d9c18 Node ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a Worker IP address: 172.28.0.12 Worker port: 40489 Worker PID: 155237 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-02-01 20:02:17,712 E 155103 155103] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "INFO flwr 2024-02-01 20:04:01,203 | server.py:125 | fit progress: (7, 1.3581448793411255, {'accuracy': 0.571399986743927}, 5627.105054081996)\n",
            "INFO:flwr:fit progress: (7, 1.3581448793411255, {'accuracy': 0.571399986743927}, 5627.105054081996)\n",
            "DEBUG flwr 2024-02-01 20:04:01,212 | server.py:173 | evaluate_round 7: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 7: strategy sampled 5 clients (out of 10)\n",
            "ERROR flwr 2024-02-01 20:04:01,722 | ray_actor.py:302 | The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "ERROR:flwr:The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "ERROR flwr 2024-02-01 20:04:01,728 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:01,739 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:01,730 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:01,732 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "WARNING flwr 2024-02-01 20:04:01,732 | ray_actor.py:323 | Actor(fa173e8fc9c625b37cec03a701000000) will be remove from pool.\n",
            "WARNING:flwr:Actor(fa173e8fc9c625b37cec03a701000000) will be remove from pool.\n",
            "ERROR flwr 2024-02-01 20:04:01,730 | ray_actor.py:302 | The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "ERROR flwr 2024-02-01 20:04:01,749 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "ERROR flwr 2024-02-01 20:04:01,751 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:01,753 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 307, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "WARNING flwr 2024-02-01 20:04:01,754 | ray_actor.py:323 | Actor(fa173e8fc9c625b37cec03a701000000) will be remove from pool.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 307, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:01,759 | ray_client_proxy.py:146 | The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "WARNING:flwr:Actor(fa173e8fc9c625b37cec03a701000000) will be remove from pool.\n",
            "ERROR flwr 2024-02-01 20:04:01,762 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 307, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "ERROR:flwr:The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 307, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:01,765 | ray_client_proxy.py:146 | The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "ERROR:flwr:The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: DefaultActor\n",
            "\tactor_id: fa173e8fc9c625b37cec03a701000000\n",
            "\tpid: 155237\n",
            "\tnamespace: 31393b15-382e-4b3b-a41b-adfbfc57baca\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "DEBUG flwr 2024-02-01 20:04:01,792 | server.py:187 | evaluate_round 7 received 0 results and 5 failures\n",
            "DEBUG:flwr:evaluate_round 7 received 0 results and 5 failures\n",
            "DEBUG flwr 2024-02-01 20:04:01,797 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 8: strategy sampled 10 clients (out of 10)\n",
            "WARNING flwr 2024-02-01 20:04:01,811 | ray_actor.py:341 | REMOVED actor fa173e8fc9c625b37cec03a701000000 from pool\n",
            "WARNING:flwr:REMOVED actor fa173e8fc9c625b37cec03a701000000 from pool\n",
            "WARNING flwr 2024-02-01 20:04:01,818 | ray_actor.py:342 | Pool size: 1\n",
            "WARNING:flwr:Pool size: 1\n",
            "ERROR flwr 2024-02-01 20:04:01,836 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2513, in get\n",
            "    raise ValueError(\n",
            "ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2513, in get\n",
            "    raise ValueError(\n",
            "ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:01,938 | ray_client_proxy.py:146 | 'object_refs' must either be an ObjectRef or a list of ObjectRefs.\n",
            "ERROR:flwr:'object_refs' must either be an ObjectRef or a list of ObjectRefs.\n",
            "ERROR flwr 2024-02-01 20:04:02,054 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:02,062 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:02,315 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:02,331 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:02,411 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:02,412 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:02,424 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:02,436 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:02,524 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:02,587 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:02,682 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:02,775 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:02,888 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:02,901 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:03,002 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:03,011 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:04:03,004 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:04:03,028 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-01 20:04:03,035 | server.py:236 | fit_round 8 received 0 results and 10 failures\n",
            "DEBUG:flwr:fit_round 8 received 0 results and 10 failures\n",
            "INFO flwr 2024-02-01 20:04:05,841 | app.py:279 | The cluster expanded. Adding 1 actors to the pool.\n",
            "INFO:flwr:The cluster expanded. Adding 1 actors to the pool.\n",
            "\u001b[2m\u001b[36m(pid=180284)\u001b[0m 2024-02-01 20:04:34.599486: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=180284)\u001b[0m 2024-02-01 20:04:34.599614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=180284)\u001b[0m 2024-02-01 20:04:34.611191: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=155237)\u001b[0m 2024-02-01 18:27:43.626166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(pid=180284)\u001b[0m 2024-02-01 20:05:19.814847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO flwr 2024-02-01 20:06:16,319 | server.py:125 | fit progress: (8, 1.3581448793411255, {'accuracy': 0.571399986743927}, 5762.220848576999)\n",
            "INFO:flwr:fit progress: (8, 1.3581448793411255, {'accuracy': 0.571399986743927}, 5762.220848576999)\n",
            "DEBUG flwr 2024-02-01 20:06:16,324 | server.py:173 | evaluate_round 8: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 8: strategy sampled 5 clients (out of 10)\n",
            "ERROR flwr 2024-02-01 20:06:16,738 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:16,848 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:16,899 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:17,006 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:17,045 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:17,049 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:17,052 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:17,053 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-01 20:06:28,707 | server.py:187 | evaluate_round 8 received 1 results and 4 failures\n",
            "DEBUG:flwr:evaluate_round 8 received 1 results and 4 failures\n",
            "DEBUG flwr 2024-02-01 20:06:28,713 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 9: strategy sampled 10 clients (out of 10)\n",
            "ERROR flwr 2024-02-01 20:06:29,164 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:29,167 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:29,191 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:29,198 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:29,362 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:29,383 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:29,533 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:29,549 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:29,780 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:29,908 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:30,011 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:30,013 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:30,016 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:30,035 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:30,125 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:30,127 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:06:30,174 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:06:30,177 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-01 20:07:32,751 | server.py:236 | fit_round 9 received 1 results and 9 failures\n",
            "DEBUG:flwr:fit_round 9 received 1 results and 9 failures\n",
            "INFO flwr 2024-02-01 20:09:56,253 | server.py:125 | fit progress: (9, 1.3709017038345337, {'accuracy': 0.535099983215332}, 5982.154145652999)\n",
            "INFO:flwr:fit progress: (9, 1.3709017038345337, {'accuracy': 0.535099983215332}, 5982.154145652999)\n",
            "DEBUG flwr 2024-02-01 20:09:56,259 | server.py:173 | evaluate_round 9: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 9: strategy sampled 5 clients (out of 10)\n",
            "ERROR flwr 2024-02-01 20:09:56,661 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:09:56,666 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:09:56,883 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:09:56,896 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:09:57,032 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:09:57,054 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:09:57,076 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:09:57,099 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-01 20:10:05,115 | server.py:187 | evaluate_round 9 received 1 results and 4 failures\n",
            "DEBUG:flwr:evaluate_round 9 received 1 results and 4 failures\n",
            "DEBUG flwr 2024-02-01 20:10:05,123 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 10: strategy sampled 10 clients (out of 10)\n",
            "ERROR flwr 2024-02-01 20:10:05,902 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:05,922 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:06,142 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:06,149 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:06,174 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:06,181 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:06,354 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:06,378 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:06,727 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:06,754 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:06,959 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:06,970 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:06,979 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:06,979 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:06,990 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:07,238 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:10:07,241 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:10:07,247 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-01 20:11:08,004 | server.py:236 | fit_round 10 received 1 results and 9 failures\n",
            "DEBUG:flwr:fit_round 10 received 1 results and 9 failures\n",
            "INFO flwr 2024-02-01 20:13:31,240 | server.py:125 | fit progress: (10, 1.355852484703064, {'accuracy': 0.5457000136375427}, 6197.1412189079965)\n",
            "INFO:flwr:fit progress: (10, 1.355852484703064, {'accuracy': 0.5457000136375427}, 6197.1412189079965)\n",
            "DEBUG flwr 2024-02-01 20:13:31,244 | server.py:173 | evaluate_round 10: strategy sampled 5 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 10: strategy sampled 5 clients (out of 10)\n",
            "ERROR flwr 2024-02-01 20:13:31,701 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:13:31,715 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:13:32,118 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:13:32,125 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:13:32,176 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:13:32,181 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-01 20:13:32,186 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-01 20:13:32,188 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 79ba9e206534aff27d3a955d993357ae28bcf88783503fb403266d2a) where the task (actor ID: 78fd82dba9db85083bf0af5b01000000, name=DefaultActor.__init__, pid=155238, memory used=3.52GB) was running was 12.07GB / 12.67GB (0.952511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4019ee3d085ccfa7651f1d386109ddd806464ac5477e5d95a53e5da7*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "155237\t3.61\t\n",
            "155238\t3.52\tray::DefaultActor\n",
            "154666\t1.78\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-2b7e4c9a-ab73...\n",
            "155005\t0.53\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:d060e6aa7d4620fd3...\n",
            "84\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "155138\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "155064\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "155063\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "155118\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "155035\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-01 20:13:39,309 | server.py:187 | evaluate_round 10 received 1 results and 4 failures\n",
            "DEBUG:flwr:evaluate_round 10 received 1 results and 4 failures\n",
            "INFO flwr 2024-02-01 20:13:39,313 | server.py:153 | FL finished in 6205.214681734997\n",
            "INFO:flwr:FL finished in 6205.214681734997\n",
            "INFO flwr 2024-02-01 20:13:39,315 | app.py:226 | app_fit: losses_distributed [(1, 1.914990496635437), (2, 1.8393938302993775), (3, 1.5678765773773193), (4, 1.4336419343948363), (5, 1.408069896697998), (6, 1.3903767585754394), (8, 1.2965540885925293), (9, 1.3360388278961182), (10, 1.2569773197174072)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 1.914990496635437), (2, 1.8393938302993775), (3, 1.5678765773773193), (4, 1.4336419343948363), (5, 1.408069896697998), (6, 1.3903767585754394), (8, 1.2965540885925293), (9, 1.3360388278961182), (10, 1.2569773197174072)]\n",
            "INFO flwr 2024-02-01 20:13:39,318 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2024-02-01 20:13:39,320 | app.py:228 | app_fit: metrics_distributed {'accuracy': [(1, 0.353600001335144), (2, 0.3856000006198883), (3, 0.4656000018119812), (4, 0.5375999927520752), (5, 0.551199996471405), (6, 0.554800009727478), (8, 0.6039999723434448), (9, 0.5659999847412109), (10, 0.5600000023841858)]}\n",
            "INFO:flwr:app_fit: metrics_distributed {'accuracy': [(1, 0.353600001335144), (2, 0.3856000006198883), (3, 0.4656000018119812), (4, 0.5375999927520752), (5, 0.551199996471405), (6, 0.554800009727478), (8, 0.6039999723434448), (9, 0.5659999847412109), (10, 0.5600000023841858)]}\n",
            "INFO flwr 2024-02-01 20:13:39,323 | app.py:229 | app_fit: losses_centralized [(0, 17.055753707885742), (1, 1.9129927158355713), (2, 1.8248263597488403), (3, 1.5837198495864868), (4, 1.4736454486846924), (5, 1.445074439048767), (6, 1.400529384613037), (7, 1.3581448793411255), (8, 1.3581448793411255), (9, 1.3709017038345337), (10, 1.355852484703064)]\n",
            "INFO:flwr:app_fit: losses_centralized [(0, 17.055753707885742), (1, 1.9129927158355713), (2, 1.8248263597488403), (3, 1.5837198495864868), (4, 1.4736454486846924), (5, 1.445074439048767), (6, 1.400529384613037), (7, 1.3581448793411255), (8, 1.3581448793411255), (9, 1.3709017038345337), (10, 1.355852484703064)]\n",
            "INFO flwr 2024-02-01 20:13:39,325 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.07819999754428864), (1, 0.3513999879360199), (2, 0.38749998807907104), (3, 0.4828000068664551), (4, 0.5307000279426575), (5, 0.5415999889373779), (6, 0.5583999752998352), (7, 0.571399986743927), (8, 0.571399986743927), (9, 0.535099983215332), (10, 0.5457000136375427)]}\n",
            "INFO:flwr:app_fit: metrics_centralized {'accuracy': [(0, 0.07819999754428864), (1, 0.3513999879360199), (2, 0.38749998807907104), (3, 0.4828000068664551), (4, 0.5307000279426575), (5, 0.5415999889373779), (6, 0.5583999752998352), (7, 0.571399986743927), (8, 0.571399986743927), (9, 0.535099983215332), (10, 0.5457000136375427)]}\n"
          ]
        }
      ],
      "source": [
        "# Enable GPU growth in your main process\n",
        "enable_tf_gpu_growth()\n",
        "\n",
        "# Download MNIST dataset and partition it\n",
        "# # mnist_fds = FederatedDataset(dataset=\"mnist\", partitioners={\"train\": NUM_CLIENTS})   # dataset=\"mnist\"\n",
        "# Download CIFAR-10 dataset and partition it\n",
        "cifar10_fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": NUM_CLIENTS})  # dataset=\"cifar10\"\n",
        "# Get the whole test set for centralised evaluation\n",
        "centralized_testset = cifar10_fds.load_full(\"test\").to_tf_dataset(\n",
        "    columns=\"img\", label_cols=\"label\", batch_size=64\n",
        ") # columns=\"image\" for mnist_fds\n",
        "\n",
        "\n",
        "# Create FedAvg strategy\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1,  # Sample 100% of available clients for training\n",
        "    fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
        "    min_fit_clients=10,  # Never sample less than 10 clients for training\n",
        "    min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
        "    min_available_clients=int(\n",
        "        NUM_CLIENTS * 0.70\n",
        "    ),  # Wait until at least 7 clients are available\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,  # aggregates federated metrics\n",
        "    evaluate_fn=get_evaluate_fn(centralized_testset),  # global evaluation function\n",
        ")\n",
        "\n",
        "# With a dictionary, you tell Flower's VirtualClientEngine that each\n",
        "# client needs exclusive access to these many resources in order to run\n",
        "client_resources = {\"num_cpus\": 1, \"num_gpus\": 0.0}\n",
        "\n",
        "# Start simulation\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=get_client_fn(cifar10_fds),\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=10),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        "    actor_kwargs={\n",
        "        \"on_actor_init_fn\": enable_tf_gpu_growth  # Enable GPU growth upon actor init.\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-csgGAAoReY"
      },
      "source": [
        "You can then use the resturned History object to either save the results to disk or do some visualisation (or both of course, or neither if you like chaos). Below you can see how you can plot the centralised accuracy obtainined at the end of each round (including at the very beginning of the experiment) for the global model. This is want the function `evaluate_fn()` that we passed to the strategy reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mUXiYQHMoReZ",
        "outputId": "44ad36fc-4a72-4bf3-a972-a0cfd7d7c256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "history.metrics_centralized = {'accuracy': [(0, 0.07819999754428864), (1, 0.3513999879360199), (2, 0.38749998807907104), (3, 0.4828000068664551), (4, 0.5307000279426575), (5, 0.5415999889373779), (6, 0.5583999752998352), (7, 0.571399986743927), (8, 0.571399986743927), (9, 0.535099983215332), (10, 0.5457000136375427)]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'cifar10 - IID - 10 clients with 10 clients per round')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"{history.metrics_centralized = }\")\n",
        "\n",
        "global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
        "round = [data[0] for data in global_accuracy_centralised]\n",
        "acc = [100.0 * data[1] for data in global_accuracy_centralised]\n",
        "plt.plot(round, acc)\n",
        "plt.grid()\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.title(\"cifar10 - IID - 10 clients with 10 clients per round\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}